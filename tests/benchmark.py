"""
BM25 æ€§èƒ½æµ‹è¯•

æµ‹è¯•ä¸åŒè§„æ¨¡æ•°æ®é›†ä¸‹çš„ç´¢å¼•å’Œæœç´¢æ€§èƒ½
"""

import time
import random
import string
from bm25 import BM25


def generate_chinese_text(length: int = 50) -> str:
    """ç”Ÿæˆéšæœºä¸­æ–‡æ–‡æœ¬"""
    # å¸¸ç”¨ä¸­æ–‡å­—ç¬¦èŒƒå›´
    chars = "".join(
        chr(i) for i in range(0x4E00, 0x9FA5) if random.random() < 0.01
    )
    if not chars:
        chars = "çš„ä¸€æ˜¯åœ¨ä¸äº†æœ‰å’Œäººè¿™ä¸­å¤§ä¸ºä¸Šä¸ªå›½æˆ‘ä»¥è¦ä»–æ—¶æ¥ç”¨ä»¬ç”Ÿåˆ°ä½œåœ°äºå‡ºå°±åˆ†å¯¹æˆä¼šå¯ä¸»å‘å¹´åŠ¨åŒå·¥ä¹Ÿèƒ½ä¸‹è¿‡å­è¯´äº§ç§é¢è€Œæ–¹åå¤šå®šè¡Œå­¦æ³•æ‰€æ°‘å¾—ç»åä¸‰ä¹‹è¿›ç€ç­‰éƒ¨åº¦å®¶ç”µåŠ›é‡Œå¦‚æ°´åŒ–é«˜è‡ªäºŒç†èµ·å°ç‰©ç°å®åŠ é‡éƒ½ä¸¤ä½“åˆ¶æœºå½“ä½¿ç‚¹ä»ä¸šæœ¬å»æŠŠæ€§å¥½åº”å¼€å®ƒåˆè¿˜å› ç”±å…¶äº›ç„¶å‰å¤–å¤©æ”¿å››æ—¥é‚£ç¤¾ä¹‰äº‹å¹³å½¢ç›¸å…¨è¡¨é—´æ ·ä¸å…³å„é‡æ–°çº¿å†…æ•°æ­£å¿ƒåä½ æ˜çœ‹åŸåˆä¹ˆåˆ©æ¯”æˆ–ä½†è´¨æ°”ç¬¬å‘é“å‘½æ­¤å˜æ¡åªæ²¡ç»“è§£é—®æ„å»ºæœˆå…¬æ— ç³»å†›å¾ˆæƒ…æœ€ä½•å‘æˆè§æ‰‹æ¬¡å·¥åœºåæˆ‘ä½“å…¨ä½†æ˜¯å¼€å§‹å¯èƒ½è¿™æ ·æ²¡æœ‰ä»€ä¹ˆä»–ä»¬æˆ‘ä»¬ä½ ä»¬è‡ªå·±çŸ¥é“æ€ä¹ˆä¸ºä»€ä¹ˆå¦‚æœç°åœ¨å·²ç»é‚£ä¹ˆæ‰€ä»¥è™½ç„¶ä½†æ˜¯å› ä¸ºå°±æ˜¯è¿™ä¸ªé‚£ä¸ªä»€ä¹ˆæ—¶å€™æ€æ ·è¿˜æ˜¯ä¸è¿‡é‚£äº›è¿™äº›ä»€ä¹ˆåœ°æ–¹"
    return "".join(random.choice(chars) for _ in range(length))


def benchmark_fit(doc_count: int, doc_length: int = 100) -> float:
    """æµ‹è¯•ç´¢å¼•æ€§èƒ½"""
    documents = [generate_chinese_text(doc_length) for _ in range(doc_count)]
    
    bm25 = BM25()
    start = time.perf_counter()
    bm25.fit(documents)
    elapsed = time.perf_counter() - start
    
    return elapsed


def benchmark_search(bm25: BM25, query: str, iterations: int = 100) -> float:
    """æµ‹è¯•æœç´¢æ€§èƒ½"""
    start = time.perf_counter()
    for _ in range(iterations):
        bm25.search(query, top_k=10)
    elapsed = time.perf_counter() - start
    
    return elapsed / iterations


def run_benchmarks():
    """è¿è¡Œå®Œæ•´çš„æ€§èƒ½æµ‹è¯•"""
    print("=" * 60)
    print("BM25 æ€§èƒ½æµ‹è¯•")
    print("=" * 60)
    
    # ç´¢å¼•æ€§èƒ½æµ‹è¯•
    print("\nğŸ“Š ç´¢å¼•æ€§èƒ½æµ‹è¯• (fit)")
    print("-" * 40)
    
    doc_counts = [100, 1000, 5000, 10000]
    for count in doc_counts:
        elapsed = benchmark_fit(count)
        rate = count / elapsed
        print(f"  {count:>6} æ–‡æ¡£: {elapsed:>6.3f}s ({rate:>8.0f} docs/s)")
    
    # æœç´¢æ€§èƒ½æµ‹è¯•
    print("\nğŸ” æœç´¢æ€§èƒ½æµ‹è¯• (search)")
    print("-" * 40)
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    documents = [generate_chinese_text(100) for _ in range(10000)]
    bm25 = BM25()
    bm25.fit(documents)
    
    queries = [
        "æœºå™¨å­¦ä¹ ",
        "è‡ªç„¶è¯­è¨€å¤„ç†",
        "Python ç¼–ç¨‹ æ•°æ® åˆ†æ",
    ]
    
    for query in queries:
        avg_time = benchmark_search(bm25, query, iterations=1000)
        qps = 1 / avg_time
        print(f"  æŸ¥è¯¢ã€Œ{query[:10]}...ã€: {avg_time*1000:.3f}ms ({qps:.0f} QPS)")
    
    # å†…å­˜æ•ˆç‡æµ‹è¯•ï¼ˆè¿‘ä¼¼ï¼‰
    print("\nğŸ’¾ è¯­æ–™åº“è§„æ¨¡æµ‹è¯•")
    print("-" * 40)
    
    sizes = [1000, 5000, 10000, 20000]
    for size in sizes:
        documents = [generate_chinese_text(50) for _ in range(size)]
        bm25 = BM25()
        
        fit_time = time.perf_counter()
        bm25.fit(documents)
        fit_elapsed = time.perf_counter() - fit_time
        
        search_time = benchmark_search(bm25, "æµ‹è¯•æŸ¥è¯¢", iterations=100)
        
        print(f"  {size:>6} æ–‡æ¡£: ç´¢å¼• {fit_elapsed:.3f}s, æœç´¢ {search_time*1000:.3f}ms")
    
    print("\n" + "=" * 60)
    print("âœ… æ€§èƒ½æµ‹è¯•å®Œæˆ")
    print("=" * 60)


if __name__ == "__main__":
    run_benchmarks()
